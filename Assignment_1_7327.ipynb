{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKoVenVVgFfe"
   },
   "source": [
    "## COMP SCI 7327 Concepts in Artificial Intelligence and Machine Learning -- Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYzOUGXEgu_K"
   },
   "source": [
    "# Task 1： Explain the basic concepts (5 marks)\n",
    "\n",
    "1. ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Please explain how the ROC curve works in the binary classification (1 mark).\n",
    "\n",
    "2. Please describe what is cross-entropy and under what circumstances cross-entropy can be used (2 marks)?\n",
    "\n",
    "4. Please explain what are the similarities and differences of L1 loss and MSE loss in K-Nearest Neighbor training (2 marks)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1: Explain ROC curves**<br />\n",
    ">\n",
    "> Receiver Operator Characteristic (ROC) curves show the trade off between False-Positive Rates (FPR) and True-Positive Rates (TPR) of a binary classification model as a function of varying the discrimination threshold. The use case is to evaluate the performance of a binary classifer. The ROC curve can conveniently show the results of many confusion matrices in a single chart where TPR is on the y-axis and FPR is on the x-axis. A 'good' ROC curve 'pushes toward the top-left quadrant of the chart. The procedure for evaluating a ROC curve to outlined below.\n",
    ">\n",
    "> Inputs required:\n",
    "> 1. Logistic regression model from a binary classifer under test\n",
    "> 2. Labelled test data (i.e. truth). \n",
    "> \n",
    "> The analysis procedure:\n",
    "> 1. Choose a classification threshold (a probabilty [0, 1])\n",
    "> 1. Create confusion matrix: count false positives, true positives, false negatives, true negatives, and evaluate their rates.\n",
    "> 1. Choose another classification threshold, repeat previous two steps.\n",
    "> 1. Plot FPR and TPR for each threshold.\n",
    "> \n",
    "> The output:\n",
    "> 1. ROC \n",
    ">\n",
    "> References:\n",
    ">* StateQuest with Josh Starmer, 'ROC and AUC, Clearly Explained!', https://www.youtube.com/watch?v=4jRBRDbJemM&ab_channel=StatQuestwithJoshStarmer\n",
    ">* Neptune AI, 'F1 Score vs. ROC AUC vs. Accuracy vs. PR AUC: Which Evaluation Metric Should You Choose?', https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 2: Describe cross-entropy and under what circumstances can it be used.**<br />\n",
    ">\n",
    "> Cross-entropy is a way to evaluate an error metric for a model. When cross-entropy is aggregated it forms the objective function (aka loss function) of the optimisation algorithm used in neural networks, which rely on gradient descent techniques. Cross-entropy employs a logarithmic function to more strongly penalise errors in prediction vs. 'truth' of a model. Cross-entropy is sometimes known as 'log loss function'.\n",
    ">\n",
    "> When is it used? <br />\n",
    "> * Cross-entropy is typically used in supervised learning binary classification problems, where the model's prediction is whether an object is or isn't a class/category.\n",
    "> * Supervised learning regression problems on the other hand, where predication is a continuous real number, typically use mean squared error or mean absolute error and therefore don't employ cross-entropy.\n",
    "> * Supervised learning multi-class classification problems typically employ soft-max cross-entropy, which is a 'normalise exponential function'.\n",
    "> \n",
    "> Why is cross-entropy used?\n",
    "> * It improves optimiser performance. An objective function (loss) function based on any variant of sum of the square of residuals doesn't adequately penalise the optimiser when residuals are large. \n",
    "> * The gradient of the objective function, which governs the optimiser's behaviour, when using cross-entropy is significantly larger vs. the use of square of residuals (or some variant of squaring errors).\n",
    "> * So 'cross-entropy helps take a relatively large step towards a better prediction' at the next optimiser step.\n",
    ">\n",
    "> References considered: \n",
    "> * StatQuest with Jos Starmer, 'Neural Networks Part 6: Cross Entropy', https://www.youtube.com/watch?v=6ArSys5qHAU&t=303s&ab_channel=StatQuestwithJoshStarmer\n",
    "> * Normalized Nerd, 'Why do we need Cross Entropy Loss? (Visualised)', https://www.youtube.com/watch?v=gIx974WtVb4&ab_channel=NormalizedNerd\n",
    "> * JD Hao, 'Why cross entropy in classification', https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/\n",
    "> * Numpy Ninja, 'Loss Functions - when to use which one?' https://www.numpyninja.com/post/loss-functions-when-to-use-which-one\n",
    "> * Super Data Science Team, 'CNN: Softmax vs. Cross-Entropy', https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-softmax-crossentropy\n",
    "> * Geoffrey Hinton, 'The softmax output function', https://www.youtube.com/watch?v=PHP8beSz5o4&ab_channel=ArtificialIntelligence-AllinOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 3: Explain the similarities and differences of L1 loss and MSE loss in K-Nearest Neighbor training (2 marks).**<br />\n",
    ">\n",
    "> What is K-nearest neighbour?\n",
    "> K-nearest neighbour is a supervised learning algorithm. It can be used in both regression and classification problems. It is considered simple and reasonably easy to implement. The disadvantage is that this algorithm can be slow if the number of data points are large. Given a datapoint in a classification problem, K is a parameter that defines how many of the nearest neighbours of this datapoint should be considered for classification. Consider the edge case: if K were 7, and of these neighbours 4 are classed as 'red and 3 and 'green, then a majority vote evaluated to classify the data point. If the vote is tied then the options are to 50/50 assign a classification or not classify at all.\n",
    ">\n",
    "> K-nearest neighbour 'loss' metrics\n",
    ">\n",
    ">\n",
    "> What is L1 loss?\n",
    "> * L1 loss is also known as Least Absolute Deviations (LAD), Least Absolute Error (LAE) or L1-norm loss. \n",
    "> * L1 loss evaluates the aboslute differences between a 'truth' value and the 'predicted' value.<br />\n",
    "> $|x_i-y_i|$\n",
    ">\n",
    "> What is MSE loss?\n",
    "> * Mean Squared Error (MSE) loss evaluates the square of the L2 norm (euclidean distance) of 'predicted' vs. 'actual' (or error vectors in the mathematical definition).<br />\n",
    "> * $\\frac{1}{n}\\sum_{i=1}^{D}(x_i-y_i)^2$ <br />\n",
    ">\n",
    "> What are the similarities?\n",
    ">\n",
    "> What are the differences?\n",
    ">\n",
    "> When should you use L1 loss vs. MSE loss in K-Nearest neighbour training?\n",
    ">\n",
    "> References considered:\n",
    "> * Onel Harrision, 'Machine Learning Basics with the K-Nearest Neighbors Algorithm', https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n",
    "> * StackExchange, 'What is the cost/loss function of a K-Nearest neighbour?', https://stats.stackexchange.com/questions/358537/what-is-the-cost-loss-function-of-k-nearest-neighbors\n",
    "> * Chase Dowling, 'L1, L2 Loss Functions and Regression', https://cpatdowling.github.io/notebooks/regression_2\n",
    "> * Lo, 'Differences between L1 and L2 as Loss Function and Regularization', https://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esyKtc4Sl0on"
   },
   "source": [
    "# Task 2 ：Python programming (8 marks)\n",
    "\n",
    "1. Given a list of numbers: num=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], write python code that finds all odd numbers in a list and returns a new list that contains all the odd numbers (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_odd_num(numbers):\n",
    "    odd_numbers = [num for num in numbers if num % 2 > 0]\n",
    "    return odd_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtRQVLwFl-Tn"
   },
   "source": [
    "2. The “carry” means if the summation of one digital position is greater than 10, an another 1 will be added to the next position. Please write python code to count the number of the “carry” operations. For example, 123+456 has no \"carry\" as neither of 3+6，2+5, 1+4 is greater than 10. Some other examples are as below (3 marks):\n",
    "\n",
    "> **Example 1:**\n",
    "\n",
    "> Input: \n",
    "> 123+456\n",
    "\n",
    "> Output:\n",
    "> No carry operation.\n",
    "\n",
    "> **Example 2:**\n",
    "\n",
    "> Input: \n",
    "> 555+555\n",
    "\n",
    "> Output:\n",
    "> 3 carry operations.\n",
    "\n",
    "> **Example 3:**\n",
    "\n",
    "> Input: \n",
    "> 123+594\n",
    "\n",
    "> Output:\n",
    "> 1 carry operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCarries(number_a: int, number_b: int) -> int:\n",
    "    \"\"\"Count the carries.\n",
    "    \n",
    "    Args:\n",
    "        number_a (int) \n",
    "        number_b (int) \n",
    "    Returns:\n",
    "        num_carries (int)\n",
    "\n",
    "    Assumptions:\n",
    "    1. Addition only.\n",
    "    2. Negative numbers excluded.\n",
    "    \"\"\"\n",
    "    num_carries = 0\n",
    "    number_c = number_a + number_b\n",
    "\n",
    "    digits_a = [int(a) for a in str(abs(number_a))]\n",
    "    digits_b = [int(b) for b in str(abs(number_b))]\n",
    "    digits_c = [int(c) for c in str(abs(number_c))]\n",
    "\n",
    "    digits_a.reverse()\n",
    "    digits_b.reverse()\n",
    "    digits_c.reverse()\n",
    "\n",
    "    num_least_digits = min(len(digits_a), len(digits_b))\n",
    "\n",
    "    for i in range(num_least_digits):\n",
    "        if digits_c[i] < digits_b[i] + digits_a[i]:\n",
    "            num_carries += 1\n",
    "\n",
    "    return num_carries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNo6I-vMmFRR"
   },
   "source": [
    "3. Roman numerals are represented by seven different symbols: I (=1) , V (=5), X(=10), L (=50), C(=100), D (=500) and M(=1000). For example, 2 is written as 'II' in Roman numeral, just two 'I' added together. The number 27 is written as XXVII, which is XX + V + II. Your task is to write a Python code that recognizes the roman numbers. The input and output should be in the format as shown below (3 marks):\n",
    "\n",
    ">**Example 1:**\n",
    "\n",
    ">Input: s = \"III\"\n",
    "\n",
    ">Output: 3\n",
    "\n",
    ">**Example 2:**\n",
    "\n",
    ">Input: s = \"LVIII\"\n",
    "\n",
    ">Output: 58\n",
    "\n",
    ">**Example 3:**\n",
    "\n",
    ">Input: s = \"MCMXCIV\"\n",
    "\n",
    ">Output: 1994\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def convert_to_numbers(roman_string: str) -> int:\n",
    "    \"\"\"Convert roman numerals to numbers.\n",
    "\n",
    "    Args:\n",
    "        roman_string (string) \n",
    "\n",
    "    Returns:\n",
    "        number (int)\n",
    "    \"\"\"\n",
    "    rome2num = {'I': 1, 'V': 5, 'X': 10,\n",
    "                'L': 50, 'C': 100, 'D': 500, 'M': 1000}\n",
    "    sum = 0\n",
    "    previous_num = math.inf\n",
    "\n",
    "    for index, char in enumerate(roman_string):\n",
    "        current_num = rome2num[char]\n",
    "        if previous_num < current_num:\n",
    "            sum -= 2*previous_num\n",
    "            sum += current_num\n",
    "        else:\n",
    "            sum += current_num\n",
    "            previous_num = current_num\n",
    "\n",
    "    return sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjTd8XaYnFLb"
   },
   "source": [
    "# Task 3 : Algorithm Programming (7 marks)\n",
    "\n",
    "1. Download the MNIST dataset and split the dataset into a training set (70% of the data), validation set (10% of the data) and testing set (20% of the data) (1 Mark).\n",
    "\n",
    "2. Build a classifier with three convolutional layers with pyTorch 1.2.0 (cpu version) (2 Marks).\n",
    "\n",
    "3. Successfully train the classifier and record the accuracy on the testing set. Please note that you will need to use all the three subsets you got in 1. (2 Marks).\n",
    "4. Please draw the loss curves and accuracy curves for both the training and validation set. You have to use Matplotlib to draw the figure.(2 Marks).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets, svm, metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Get MNIST via sklearn\n",
    "# X, y = datasets.fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "# # X is the data set and y are its labels. If return_X_y=False, then type is sklearn 'Bunch', an extension of dictionaries.\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_delta, y_train, y_delta = train_test_split(X, y, train_size= 0.7, test_size=0.3)\n",
    "# X_validation, X_test, y_validation, y_test = train_test_split(X_delta, y_delta, train_size= 1/3, test_size=2/3)\n",
    "\n",
    "# # Demonstrate that the dataset has been split: 70% training, 10% validation, 20% training\n",
    "# print(f'validation_set: {len(X_validation)}, test_set: {len(X_test)}, training_set: {len(X_train)}')\n",
    "# print(f'validation_set: {len(X_validation)/len(X)}, test_set: {len(X_test)/len(X)}, training_set: {len(X_train)/len(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is now 70000 data instances in all_data.\n",
      "There is now 49000 data instances in train_data_resliced (70% of all_data).\n",
      "There is now 14000 data instances in test_data_resliced (20% of all_data).\n",
      "There is now 7000 data instances in validation_data (10% of all_data).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "# Concatenate MNIST training and test set\n",
    "all_data = torch.utils.data.ConcatDataset([train_data, test_data])\n",
    "print(f'There is now {len(all_data)} data instances in all_data.')\n",
    "\n",
    "# Recut all_data: 70% training, 20% test, 10% validation.\n",
    "# Lesson leartn: can't naively slice pytorch data class. Use Subset method.\n",
    "idx_train = int(len(all_data)*0.7)\n",
    "idx_test = idx_train + int(len(all_data)*0.2)\n",
    "idx_validation = len(all_data)\n",
    "\n",
    "train_indices = list(range(0, idx_train))\n",
    "train_data_resliced = torch.utils.data.Subset(all_data, train_indices)\n",
    "print(f'There is now {len(train_data_resliced)} data instances in train_data_resliced (70% of all_data).')\n",
    "\n",
    "test_indices = list(range(idx_train, idx_test))\n",
    "test_data_resliced = torch.utils.data.Subset(all_data, test_indices)\n",
    "print(f'There is now {len(test_data_resliced)} data instances in test_data_resliced (20% of all_data).')\n",
    "\n",
    "validation_indices = list(range(idx_test, idx_validation))\n",
    "validation_data = torch.utils.data.Subset(all_data, validation_indices)\n",
    "print(f'There is now {len(validation_data)} data instances in validation_data (10% of all_data).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290b3f437c8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot an instance of the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_data[5][0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define models\n",
    "class NeuralNetworkLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetworkLinear, self).__init__()\n",
    "        self.flatten = nn.Flatten() # converts 2D arrays into one contiguous array.\n",
    "        self.linear_relu_stack = nn.Sequential( \n",
    "            nn.Linear(28*28, 64), # args for CNN: input channels, output channels, kernel_size\n",
    "            nn.ReLU(), # an activation layer that introduces non-linearities to the linear model\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def create_CNN_model():\n",
    "    model = nn.Sequential(\n",
    "\n",
    "        nn.Conv2d(1, 6, 5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "        nn.Conv2d(6, 16, 5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "        nn.Conv2d(6, 16, 5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(400, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Pytorch loader\n",
    "train_loader = DataLoader(train_data_resliced, batch_size=32)\n",
    "validation_loader = DataLoader(validation_data, batch_size=32)\n",
    "test_loader = DataLoader(test_data_resliced, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate(model, data):\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     for i, (images, labels) in enumerate(data):\n",
    "#         x = model(images)\n",
    "#         value, pred = torch.max(x, 1)\n",
    "#         pred = pred.data.cpu()\n",
    "#         total += x.size(0)\n",
    "#         correct += torch.sum(pred == labels)\n",
    "#     return float(correct*100./total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and validation loops\n",
    "\n",
    "# References used\n",
    "# Pytorch: Quickstart: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "# PyTorch Lightning: https://www.youtube.com/watch?v=OMDn66kM9Qc&ab_channel=PyTorchLightning\n",
    "# Krishna Ramesh: https://www.youtube.com/watch?v=ijaT8HuCtIY&ab_channel=KrishnaRamesh \n",
    "\n",
    "def train(dataloader, model, loss_function, optimiser):\n",
    "\n",
    "    # import copy\n",
    "\n",
    "    # Train the model\n",
    "    # num_epochs = 5\n",
    "    # max_accuracy = 0\n",
    "    # losses = list()\n",
    "    # accuracies = list()\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # X, y = X.to(device), y.to(device) # where x is image and y is label\n",
    "\n",
    "        # # Create matrix (why?)\n",
    "        # num_rows = X.size(0) # The batch size\n",
    "        # A = X.view(num_rows, -1) # Creates a matrix whose num_rows is batch size, num_columns = 28x28\n",
    "\n",
    "        # Compute predication error\n",
    "        logits = model(X) # Predict the result given this images\n",
    "        J = loss_function(logits, y)\n",
    "\n",
    "        # Backpropagration\n",
    "        model.zero_grad() # Equivalent to optimser.zero_grad() or params.grad._zero()\n",
    "        J.backward() # Accumulate the partial derivatives of J wrt params. Equivalent to params.grad._sum(dJ/dparams)\n",
    "        optimiser.step() # Step in the opposite direction of the gradient\n",
    "\n",
    "        # # Store loss\n",
    "        # losses.append(J.item())\n",
    "        \n",
    "        # Evaluate and store accuracy and store model\n",
    "        # accuracy = validate(model, validation_data)\n",
    "        # accuracies.append(accuracy)\n",
    "        # if accuracy > max_accuracy:\n",
    "        #     best_model = copy.deepcopy(model)\n",
    "        #     max_accuracy = accuracy\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = J.item(), batch * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12340/2177830471.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_resliced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_resliced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12340/1565051010.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_function, optimiser)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Compute predication error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Predict the result given this images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Backpropagration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uni_adelaide_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uni_adelaide_torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uni_adelaide_torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uni_adelaide_torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1818\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1820\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1821\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = NeuralNetworkLinear()\n",
    "optimiser = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_function = nn.CrossEntropyLoss() # This defines how the optimiser changes the NN parameters\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_data_resliced, model, loss_function, optimiser)\n",
    "    test(test_data_resliced, model, loss_function)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_1_7327.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
