{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKoVenVVgFfe"
   },
   "source": [
    "## COMP SCI 7327 Concepts in Artificial Intelligence and Machine Learning -- Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYzOUGXEgu_K"
   },
   "source": [
    "# Task 1： Explain the basic concepts (5 marks)\n",
    "\n",
    "1. ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. Please explain how the ROC curve works in the binary classification (1 mark).\n",
    "\n",
    "2. Please describe what is cross-entropy and under what circumstances cross-entropy can be used (2 marks)?\n",
    "\n",
    "4. Please explain what are the similarities and differences of L1 loss and MSE loss in training a classifier (2 marks)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1: Explain ROC curves**<br />\n",
    ">\n",
    "> Receiver Operator Characteristic (ROC) curves show the trade off between False-Positive Rates (FPR) and True-Positive Rates (TPR) of a binary classification model as a function of varying the discrimination threshold. The use case is to evaluate the performance of a binary classifier. The ROC curve can conveniently show the results of many confusion matrices in a single chart where TPR is on the y-axis and FPR is on the x-axis. A 'good' ROC curve 'pushes toward the top-left quadrant of the chart. The procedure for evaluating a ROC curve to outlined below.\n",
    ">\n",
    "> Inputs required:\n",
    "> 1. Logistic regression model from a binary classifier under test\n",
    "> 2. Labelled test data (i.e. truth). \n",
    "> \n",
    "> The analysis procedure:\n",
    "> 1. Choose a classification threshold, which is a probabilty (0, 1).\n",
    "> 1. Create confusion matrix: count false positives, true positives, false negatives, true negatives, and evaluate their rates.\n",
    "> 1. Choose another classification threshold, repeat previous two steps.\n",
    "> 1. Plot FPR and TPR for each threshold.\n",
    "> \n",
    "> The output:\n",
    "> 1. ROC chart.\n",
    ">\n",
    "> References:\n",
    ">* StatQuest with Josh Starmer, 'ROC and AUC, Clearly Explained!', https://www.youtube.com/watch?v=4jRBRDbJemM&ab_channel=StatQuestwithJoshStarmer\n",
    ">* Neptune AI, 'F1 Score vs. ROC AUC vs. Accuracy vs. PR AUC: Which Evaluation Metric Should You Choose?', https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 2: Describe cross-entropy and under what circumstances can it be used.**<br />\n",
    ">\n",
    "> Cross-entropy is a way to evaluate an error metric for a supervised learning binary classification model. When cross-entropy is aggregated it forms the result of the objective function (aka loss function) of the optimisation algorithm used in neural networks, which relies on gradient descent techniques. Cross-entropy employs a logarithmic function to more strongly penalise errors in prediction of the model vs. 'truth'. Cross-entropy is sometimes known as 'log loss function'.\n",
    ">\n",
    "> When is it used? <br />\n",
    "> * Cross-entropy is typically used in supervised learning binary classification problems, where the model's prediction is determine whether an object is or isn't a class/category.\n",
    "> * Supervised learning regression problems on the other hand, where predication is a continuous real number, typically use mean squared error or mean absolute error and therefore don't employ cross-entropy.\n",
    "> * Supervised learning multi-class classification problems typically employ soft-max cross-entropy, which is a 'normalise exponential function', and typically don't employ cross-entropy.\n",
    "> \n",
    "> Why is cross-entropy used?\n",
    "> * It improves optimiser performance. An objective function (loss) function based on any variant of sum of the square of residuals doesn't adequately penalise the optimiser when residuals are large. \n",
    "> * The gradient of the objective function, which governs the optimiser's behaviour, when using cross-entropy is significantly larger vs.  square of residuals (or some variant of squaring errors).\n",
    "> * So 'cross-entropy helps take a relatively large step towards a better prediction' at the next optimiser step.\n",
    ">\n",
    "> References considered: \n",
    "> * StatQuest with Jos Starmer, 'Neural Networks Part 6: Cross Entropy', https://www.youtube.com/watch?v=6ArSys5qHAU&t=303s&ab_channel=StatQuestwithJoshStarmer\n",
    "> * Normalized Nerd, 'Why do we need Cross Entropy Loss? (Visualised)', https://www.youtube.com/watch?v=gIx974WtVb4&ab_channel=NormalizedNerd\n",
    "> * JD Hao, 'Why cross entropy in classification', https://jdhao.github.io/2021/10/16/why_cross_entropy_in_classification/\n",
    "> * Numpy Ninja, 'Loss Functions - when to use which one?' https://www.numpyninja.com/post/loss-functions-when-to-use-which-one\n",
    "> * Super Data Science Team, 'CNN: Softmax vs. Cross-Entropy', https://www.superdatascience.com/blogs/convolutional-neural-networks-cnn-softmax-crossentropy\n",
    "> * Geoffrey Hinton, 'The softmax output function', https://www.youtube.com/watch?v=PHP8beSz5o4&ab_channel=ArtificialIntelligence-AllinOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 3: Explain the similarities and differences of L1 loss and MSE loss in training a classifier (2 marks).**<br />\n",
    ">\n",
    "> A classifier is a supervised learning problem. In general, a classifier evaluates a cost function. L1 and MSE loss are two different formulations of a cost function.\n",
    ">\n",
    "> What is L1 loss?\n",
    "> * L1 loss is also known as Least Absolute Deviations (LAD), Least Absolute Error (LAE) or L1-norm loss. \n",
    "> * L1 loss evaluates the absolute differences between a 'truth' value and the 'predicted' value $\\sum_{i=1}^{n}|x_i-y_i|$ <br />\n",
    "> * In the context of K-nearest neighbours, this is equivalent to the 'manhattanDistance'.\n",
    ">\n",
    "> What is MSE loss?\n",
    "> * Mean Squared Error (MSE) loss evaluates the mean/average of the sum of the squares of differences between 'predicted' vs. 'actual'. $\\frac{1}{n}\\sum_{i=1}^{n}(x_i-y_i)^2$ <br />\n",
    ">\n",
    "> What are the similarities?\n",
    "> * Both evaluate distance.\n",
    ">\n",
    "> What are the differences?\n",
    "> * L1 loss is 'less sensitive to outliers but harder to optimize'.\n",
    "> * 'L2 loss is less forgiving than L1' due to the squaring of the errors.\n",
    ">\n",
    "> References considered:\n",
    "> * Data Vedas, 'K-Nearest Neighbours', https://www.datavedas.com/k-nearest-neighbors/\n",
    "> * David Inouye, 'Loss Functions and Regularization', https://www.davidinouye.com/course/ece57000-fall-2020/lectures/loss-functions-and-regularization.pdf\n",
    "> * Data Course, 'Evaluation of Regression Models in SciKitLearn', https://www.datacourses.com/evaluation-of-regression-models-in-scikit-learn-846/\n",
    "> * Onel Harrision, 'Machine Learning Basics with the K-Nearest Neighbors Algorithm', https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n",
    "> * StackExchange, 'What is the cost/loss function of a K-Nearest neighbour?', https://stats.stackexchange.com/questions/358537/what-is-the-cost-loss-function-of-k-nearest-neighbors\n",
    "> * Chase Dowling, 'L1, L2 Loss Functions and Regression', https://cpatdowling.github.io/notebooks/regression_2\n",
    "> * Lo, 'Differences between L1 and L2 as Loss Function and Regularization', https://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "> * Bio Statics Australia, 'K-Nearest Neighbour Regression', https://bookdown.org/tpinto_home/Regression-and-Classification/k-nearest-neighbours-regression.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esyKtc4Sl0on"
   },
   "source": [
    "# Task 2 ：Python programming (8 marks)\n",
    "\n",
    "1. Given a list of numbers: num=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], write python code that finds all odd numbers in a list and returns a new list that contains all the odd numbers (2 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtRQVLwFl-Tn"
   },
   "source": [
    "2. The “carry” means if the summation of one digital position is greater than 10, an another 1 will be added to the next position. Please write python code to count the number of the “carry” operations. For example, 123+456 has no \"carry\" as neither of 3+6，2+5, 1+4 is greater than 10. Some other examples are as below (3 marks):\n",
    "\n",
    "> **Example 1:**\n",
    "\n",
    "> Input: \n",
    "> 123+456\n",
    "\n",
    "> Output:\n",
    "> No carry operation.\n",
    "\n",
    "> **Example 2:**\n",
    "\n",
    "> Input: \n",
    "> 555+555\n",
    "\n",
    "> Output:\n",
    "> 3 carry operations.\n",
    "\n",
    "> **Example 3:**\n",
    "\n",
    "> Input: \n",
    "> 123+594\n",
    "\n",
    "> Output:\n",
    "> 1 carry operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNo6I-vMmFRR"
   },
   "source": [
    "3. Roman numerals are represented by seven different symbols: I (=1) , V (=5), X(=10), L (=50), C(=100), D (=500) and M(=1000). For example, 2 is written as 'II' in Roman numeral, just two 'I' added together. The number 27 is written as XXVII, which is XX + V + II. Your task is to write a Python code that recognizes the roman numbers. The input and output should be in the format as shown below (3 marks):\n",
    "\n",
    ">**Example 1:**\n",
    "\n",
    ">Input: s = \"III\"\n",
    "\n",
    ">Output: 3\n",
    "\n",
    ">**Example 2:**\n",
    "\n",
    ">Input: s = \"LVIII\"\n",
    "\n",
    ">Output: 58\n",
    "\n",
    ">**Example 3:**\n",
    "\n",
    ">Input: s = \"MCMXCIV\"\n",
    "\n",
    ">Output: 1994\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the code in the assignment1 folder.\n",
    "* task2_print_odd_num.py\n",
    "* task2_count_carries.py\n",
    "* task2_romain_numbers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjTd8XaYnFLb"
   },
   "source": [
    "# Task 3 : Algorithm Programming (7 marks)\n",
    "\n",
    "1. Download the MNIST dataset and split the dataset into a training set (70% of the data), validation set (10% of the data) and testing set (20% of the data) (1 Mark).\n",
    "\n",
    "2. Build a classifier with three convolutional layers with pyTorch 1.2.0 (cpu version) (2 Marks).\n",
    "\n",
    "3. Successfully train the classifier and record the accuracy on the testing set. Please note that you will need to use all the three subsets you got in 1. (2 Marks).\n",
    "4. Please draw the loss curves and accuracy curves for both the training and validation set. You have to use Matplotlib to draw the figure.(2 Marks).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the code in the assignment1 folder.\n",
    "* task3_pytorch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_1_7327.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
